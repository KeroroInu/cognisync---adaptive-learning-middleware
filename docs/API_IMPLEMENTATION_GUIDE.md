# CogniSync åç«¯ API å®ç°æŒ‡å—

## ç›®æ ‡è¯»è€…
æœ¬æ–‡æ¡£é¢å‘åç«¯å·¥ç¨‹å¸ˆï¼Œæä¾›æ¸…æ™°çš„ API å®ç°æ­¥éª¤å’Œä»£ç ç¤ºä¾‹ã€‚

---

## å¿«é€Ÿå¼€å§‹

### 1. æŠ€æœ¯æ ˆé€‰æ‹©

æ¨èä½¿ç”¨ **FastAPI** (Python):
- è‡ªåŠ¨ç”Ÿæˆ OpenAPI æ–‡æ¡£
- å†…ç½®ç±»å‹éªŒè¯ (Pydantic)
- å¼‚æ­¥æ”¯æŒ
- æ€§èƒ½ä¼˜ç§€

### 2. é¡¹ç›®åˆå§‹åŒ–

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir cognisync-backend
cd cognisync-backend

# åˆå§‹åŒ– Python ç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install fastapi uvicorn sqlalchemy psycopg2-binary alembic redis openai python-jose
```

### 3. ç›®å½•ç»“æ„

```
cognisync-backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI åº”ç”¨å…¥å£
â”‚   â”œâ”€â”€ config.py               # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ database.py             # æ•°æ®åº“è¿æ¥
â”‚   â”œâ”€â”€ models/                 # SQLAlchemy æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ user_profile.py
â”‚   â”‚   â”œâ”€â”€ knowledge_node.py
â”‚   â”‚   â”œâ”€â”€ chat_message.py
â”‚   â”‚   â””â”€â”€ calibration_log.py
â”‚   â”œâ”€â”€ schemas/                # Pydantic æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ profile.py
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”œâ”€â”€ knowledge_graph.py
â”‚   â”‚   â””â”€â”€ calibration.py
â”‚   â”œâ”€â”€ api/                    # API è·¯ç”±
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ profile.py
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”œâ”€â”€ knowledge_graph.py
â”‚   â”‚   â””â”€â”€ calibration.py
â”‚   â”œâ”€â”€ services/               # ä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ profile_service.py
â”‚   â”‚   â”œâ”€â”€ chat_service.py
â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â””â”€â”€ knowledge_service.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ concept_matcher.py
â”œâ”€â”€ alembic/                    # æ•°æ®åº“è¿ç§»
â”œâ”€â”€ tests/
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## æ ¸å¿ƒå®ç°æ­¥éª¤

### Step 1: é…ç½®ç®¡ç† (app/config.py)

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # æ•°æ®åº“é…ç½®
    DATABASE_URL: str = "postgresql://user:pass@localhost/cognisync"

    # Redis é…ç½®
    REDIS_URL: str = "redis://localhost:6379/0"

    # OpenAI é…ç½®
    OPENAI_API_KEY: str
    OPENAI_MODEL: str = "gpt-4"

    # åº”ç”¨é…ç½®
    SECRET_KEY: str
    DEBUG: bool = False
    CORS_ORIGINS: list[str] = ["http://localhost:3000"]

    class Config:
        env_file = ".env"

settings = Settings()
```

### Step 2: æ•°æ®åº“æ¨¡å‹ (app/models/user_profile.py)

```python
from sqlalchemy import Column, String, Integer, DateTime
from sqlalchemy.sql import func
from app.database import Base

class UserProfile(Base):
    __tablename__ = "user_profiles"

    user_id = Column(String(255), primary_key=True)
    cognition = Column(Integer, nullable=False)
    affect = Column(Integer, nullable=False)
    behavior = Column(Integer, nullable=False)
    last_update = Column(DateTime, nullable=False)
    created_at = Column(DateTime, server_default=func.now())
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now())
```

### Step 3: Pydantic Schemas (app/schemas/profile.py)

```python
from pydantic import BaseModel, Field
from datetime import datetime

class ProfileBase(BaseModel):
    cognition: int = Field(ge=0, le=100)
    affect: int = Field(ge=0, le=100)
    behavior: int = Field(ge=0, le=100)

class ProfileCreate(ProfileBase):
    user_id: str

class ProfileResponse(ProfileBase):
    user_id: str
    last_update: datetime

    class Config:
        from_attributes = True

class ProfileDelta(BaseModel):
    cognition: int = 0
    affect: int = 0
    behavior: int = 0
```

### Step 4: ä¸šåŠ¡é€»è¾‘å±‚ (app/services/profile_service.py)

```python
from sqlalchemy.orm import Session
from app.models.user_profile import UserProfile
from app.schemas.profile import ProfileCreate, ProfileDelta
from datetime import datetime

class ProfileService:
    def __init__(self, db: Session):
        self.db = db

    def get_profile(self, user_id: str) -> UserProfile | None:
        return self.db.query(UserProfile).filter(
            UserProfile.user_id == user_id
        ).first()

    def create_profile(self, profile_data: ProfileCreate) -> UserProfile:
        profile = UserProfile(
            user_id=profile_data.user_id,
            cognition=profile_data.cognition,
            affect=profile_data.affect,
            behavior=profile_data.behavior,
            last_update=datetime.utcnow()
        )
        self.db.add(profile)
        self.db.commit()
        self.db.refresh(profile)
        return profile

    def update_profile(
        self,
        user_id: str,
        delta: ProfileDelta
    ) -> UserProfile:
        profile = self.get_profile(user_id)
        if not profile:
            raise ValueError(f"Profile not found for user_id: {user_id}")

        # åº”ç”¨å¢é‡å¹¶é™åˆ¶èŒƒå›´
        profile.cognition = max(0, min(100, profile.cognition + delta.cognition))
        profile.affect = max(0, min(100, profile.affect + delta.affect))
        profile.behavior = max(0, min(100, profile.behavior + delta.behavior))
        profile.last_update = datetime.utcnow()

        self.db.commit()
        self.db.refresh(profile)
        return profile
```

### Step 5: LLM åˆ†ææœåŠ¡ (app/services/llm_service.py)

```python
import openai
import json
from app.config import settings
from app.schemas.chat import MessageAnalysis

openai.api_key = settings.OPENAI_API_KEY

class LLMService:
    @staticmethod
    def analyze_message(
        user_message: str,
        current_profile: dict
    ) -> MessageAnalysis:
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ•™è‚²å¿ƒç†å­¦ä¸“å®¶ï¼Œè´Ÿè´£åˆ†æå­¦ç”Ÿçš„å­¦ä¹ æ¶ˆæ¯ã€‚

å½“å‰å­¦ç”Ÿç”»åƒ:
- è®¤çŸ¥æ°´å¹³ (0-100): {current_profile['cognition']}
- æƒ…æ„ŸçŠ¶æ€ (0-100): {current_profile['affect']}
- è¡Œä¸ºå‚ä¸ (0-100): {current_profile['behavior']}

å­¦ç”Ÿæ¶ˆæ¯: "{user_message}"

è¯·åˆ†æä»¥ä¸‹å†…å®¹å¹¶ä»¥ JSON æ ¼å¼è¿”å›:
{{
  "intent": "help-seeking | confirmation | question | statement",
  "emotion": "confused | confident | frustrated | neutral",
  "detectedConcepts": ["æ¦‚å¿µ1", "æ¦‚å¿µ2"],
  "profileDelta": {{
    "cognition": 0,
    "affect": 0,
    "behavior": 0
  }}
}}

è§„åˆ™:
- intent: help-seeking (æ±‚åŠ©), confirmation (ç¡®è®¤ç†è§£), question (æé—®), statement (é™ˆè¿°)
- emotion: confused (å›°æƒ‘), confident (è‡ªä¿¡), frustrated (æŒ«è´¥), neutral (ä¸­ç«‹)
- detectedConcepts: æå–æ¶ˆæ¯ä¸­æåˆ°çš„æŠ€æœ¯æ¦‚å¿µ
- profileDelta: å»ºè®®çš„ç”»åƒè°ƒæ•´å€¼ (-20 åˆ° +20)
  - å›°æƒ‘æ—¶é™ä½ cognition å’Œ affect
  - ç†è§£æ—¶æå‡ cognition å’Œ affect
  - æé—®æ—¶æå‡ behavior
"""

        try:
            response = openai.ChatCompletion.create(
                model=settings.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æ•™è‚²åˆ†æåŠ©æ‰‹ï¼Œåªè¿”å› JSONã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=300
            )

            result = json.loads(response.choices[0].message.content)
            return MessageAnalysis(**result)

        except Exception as e:
            # é™çº§åˆ°è§„åˆ™åŒ¹é…
            return LLMService._fallback_analysis(user_message)

    @staticmethod
    def _fallback_analysis(message: str) -> MessageAnalysis:
        """è§„åˆ™åŒ¹é…é™çº§æ–¹æ¡ˆ"""
        message_lower = message.lower()

        # æ„å›¾è¯†åˆ«
        intent = "statement"
        if any(word in message_lower for word in ["ä¸æ‡‚", "å›°æƒ‘", "ä¸ç†è§£", "éš¾"]):
            intent = "help-seeking"
        elif any(word in message_lower for word in ["æ˜ç™½", "ç†è§£", "æ˜¯çš„", "å¥½çš„"]):
            intent = "confirmation"
        elif "?" in message or "ï¼Ÿ" in message:
            intent = "question"

        # æƒ…æ„Ÿè¯†åˆ«
        emotion = "neutral"
        if intent == "help-seeking":
            emotion = "confused"
        elif intent == "confirmation":
            emotion = "confident"

        # æ¦‚å¿µæå– (ç®€åŒ–ç‰ˆ)
        concepts = []
        keywords = ["ç¥ç»ç½‘ç»œ", "åå‘ä¼ æ’­", "æ¢¯åº¦ä¸‹é™", "è¿‡æ‹Ÿåˆ", "æ¿€æ´»å‡½æ•°"]
        for kw in keywords:
            if kw in message:
                concepts.append(kw)

        # ç”»åƒå¢é‡
        delta = {"cognition": 0, "affect": 0, "behavior": 2}
        if emotion == "confused":
            delta = {"cognition": -5, "affect": -10, "behavior": 5}
        elif emotion == "confident":
            delta = {"cognition": 8, "affect": 5, "behavior": 2}

        return MessageAnalysis(
            intent=intent,
            emotion=emotion,
            detectedConcepts=concepts,
            profileDelta=delta
        )

    @staticmethod
    def generate_reply(
        user_message: str,
        analysis: MessageAnalysis,
        current_profile: dict
    ) -> str:
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI å­¦ä¹ å¯¼å¸ˆã€‚

å­¦ç”Ÿæ¶ˆæ¯: "{user_message}"
å­¦ç”ŸçŠ¶æ€: {analysis.emotion}
å­¦ç”Ÿæ„å›¾: {analysis.intent}
æåˆ°çš„æ¦‚å¿µ: {analysis.detectedConcepts}

è¯·ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„å›å¤ (50-100 å­—)ï¼Œè¦æ±‚:
- å¦‚æœå­¦ç”Ÿå›°æƒ‘ï¼Œæä¾›ç®€æ´çš„è§£é‡Šæˆ–å¼•å¯¼
- å¦‚æœå­¦ç”Ÿè‡ªä¿¡ï¼Œç»™äºˆè‚¯å®šå¹¶å¼•å…¥æ–°è¯é¢˜
- ä¿æŒé¼“åŠ±å’Œæ”¯æŒçš„è¯­æ°”
"""

        response = openai.ChatCompletion.create(
            model=settings.OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å‹å¥½çš„å­¦ä¹ å¯¼å¸ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=200
        )

        return response.choices[0].message.content
```

### Step 6: å¯¹è¯æœåŠ¡ (app/services/chat_service.py)

```python
from sqlalchemy.orm import Session
from app.models.chat_message import ChatMessage
from app.schemas.chat import ChatRequest, ChatResponse
from app.services.llm_service import LLMService
from app.services.profile_service import ProfileService
from app.services.knowledge_service import KnowledgeService
from datetime import datetime
import uuid

class ChatService:
    def __init__(self, db: Session):
        self.db = db
        self.llm_service = LLMService()
        self.profile_service = ProfileService(db)
        self.knowledge_service = KnowledgeService(db)

    def process_message(self, request: ChatRequest) -> ChatResponse:
        # 1. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        user_msg = self._save_message(
            user_id=request.userId,
            session_id=request.sessionId,
            role="user",
            message=request.message
        )

        # 2. è·å–å½“å‰ç”»åƒ
        profile = self.profile_service.get_profile(request.userId)
        if not profile:
            raise ValueError(f"Profile not found for user_id: {request.userId}")

        profile_dict = {
            "cognition": profile.cognition,
            "affect": profile.affect,
            "behavior": profile.behavior
        }

        # 3. è°ƒç”¨ LLM åˆ†æ
        analysis = self.llm_service.analyze_message(
            request.message,
            profile_dict
        )

        # 4. æ›´æ–°ç”»åƒ
        updated_profile = self.profile_service.update_profile(
            request.userId,
            analysis.profileDelta
        )

        # 5. æ›´æ–°çŸ¥è¯†å›¾è°±
        for concept in analysis.detectedConcepts:
            self.knowledge_service.increment_concept_frequency(
                request.userId,
                concept
            )

        # 6. ç”Ÿæˆå›å¤
        reply_text = self.llm_service.generate_reply(
            request.message,
            analysis,
            profile_dict
        )

        # 7. ä¿å­˜ AI æ¶ˆæ¯
        ai_msg = self._save_message(
            user_id=request.userId,
            session_id=request.sessionId,
            role="assistant",
            message=reply_text,
            analysis=analysis.dict()
        )

        # 8. è¿”å›å“åº”
        return ChatResponse(
            messageId=ai_msg.id,
            reply=reply_text,
            analysis=analysis,
            updatedProfile={
                "cognition": updated_profile.cognition,
                "affect": updated_profile.affect,
                "behavior": updated_profile.behavior
            },
            timestamp=ai_msg.timestamp
        )

    def _save_message(
        self,
        user_id: str,
        session_id: str,
        role: str,
        message: str,
        analysis: dict = None
    ) -> ChatMessage:
        msg = ChatMessage(
            id=f"msg_{uuid.uuid4().hex[:12]}",
            user_id=user_id,
            session_id=session_id,
            role=role,
            message=message,
            analysis=analysis,
            timestamp=datetime.utcnow()
        )
        self.db.add(msg)
        self.db.commit()
        self.db.refresh(msg)
        return msg
```

### Step 7: API è·¯ç”± (app/api/chat.py)

```python
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from app.database import get_db
from app.schemas.chat import ChatRequest, ChatResponse
from app.services.chat_service import ChatService

router = APIRouter(prefix="/api/chat", tags=["chat"])

@router.post("", response_model=ChatResponse)
def send_message(
    request: ChatRequest,
    db: Session = Depends(get_db)
):
    """
    å‘é€æ¶ˆæ¯å¹¶è·å– AI å›å¤

    **æµç¨‹**:
    1. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    2. LLM åˆ†æ (æ„å›¾/æƒ…æ„Ÿ/æ¦‚å¿µ)
    3. æ›´æ–°ç”¨æˆ·ç”»åƒ
    4. æ›´æ–°çŸ¥è¯†å›¾è°±é¢‘ç‡
    5. ç”Ÿæˆ AI å›å¤
    6. è¿”å›åˆ†æç»“æœ
    """
    try:
        service = ChatService(db)
        response = service.process_message(request)
        return response
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal error: {str(e)}")

@router.get("/history/{user_id}")
def get_chat_history(
    user_id: str,
    session_id: str = None,
    limit: int = 50,
    db: Session = Depends(get_db)
):
    """è·å–å¯¹è¯å†å²"""
    query = db.query(ChatMessage).filter(ChatMessage.user_id == user_id)

    if session_id:
        query = query.filter(ChatMessage.session_id == session_id)

    messages = query.order_by(ChatMessage.timestamp.desc()).limit(limit).all()

    return {
        "messages": [
            {
                "id": msg.id,
                "role": msg.role,
                "message": msg.message,
                "timestamp": msg.timestamp,
                "analysis": msg.analysis
            }
            for msg in messages
        ]
    }
```

### Step 8: ä¸»åº”ç”¨å…¥å£ (app/main.py)

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.config import settings
from app.database import engine, Base
from app.api import profile, chat, knowledge_graph, calibration

# åˆ›å»ºæ•°æ®åº“è¡¨
Base.metadata.create_all(bind=engine)

app = FastAPI(
    title="CogniSync API",
    description="Educational Agent Middleware",
    version="1.0.0"
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œè·¯ç”±
app.include_router(profile.router)
app.include_router(chat.router)
app.include_router(knowledge_graph.router)
app.include_router(calibration.router)

@app.get("/")
def root():
    return {
        "name": "CogniSync API",
        "version": "1.0.0",
        "docs": "/docs"
    }

@app.get("/health")
def health_check():
    return {"status": "ok"}
```

### Step 9: è¿è¡Œåº”ç”¨

```bash
# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
uvicorn app.main:app --reload --port 8000

# è®¿é—®æ–‡æ¡£
# http://localhost:8000/docs
```

---

## å…³é”®æ¥å£å®ç°æ¸…å•

### âœ… P0 æ¥å£ (å¿…é¡»å®ç°)

- [x] `POST /api/chat` - å¯¹è¯äº¤äº’
- [x] `GET /api/profile/:userId` - è·å–ç”»åƒ
- [ ] `POST /api/profile` - åˆ›å»ºç”»åƒ
- [ ] `GET /api/knowledge-graph/:userId` - è·å–çŸ¥è¯†å›¾è°±
- [ ] `PUT /api/knowledge-graph/nodes/:nodeId` - æ›´æ–°èŠ‚ç‚¹
- [ ] `POST /api/calibration/profile` - æäº¤ç”»åƒæ ¡å‡†

### ğŸ“‹ P1 æ¥å£ (åç»­å®ç°)

- [ ] `GET /api/chat/history/:userId` - å¯¹è¯å†å²
- [ ] `GET /api/calibration/logs/:userId` - æ ¡å‡†æ—¥å¿—
- [ ] `GET /api/export/:userId` - å¯¼å‡ºæ•°æ®

---

## æµ‹è¯•ç¤ºä¾‹

### æµ‹è¯•å¯¹è¯æ¥å£

```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "userId": "user_123",
    "message": "ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿ",
    "sessionId": "session_abc",
    "timestamp": "2025-01-19T10:30:00Z"
  }'
```

**é¢„æœŸå“åº”**:
```json
{
  "messageId": "msg_def456",
  "reply": "è¿‡æ‹Ÿåˆæ˜¯æŒ‡æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šæ³›åŒ–èƒ½åŠ›å·®çš„ç°è±¡...",
  "analysis": {
    "intent": "question",
    "emotion": "neutral",
    "detectedConcepts": ["è¿‡æ‹Ÿåˆ"],
    "profileDelta": {
      "cognition": 2,
      "affect": 0,
      "behavior": 5
    }
  },
  "updatedProfile": {
    "cognition": 67,
    "affect": 42,
    "behavior": 83
  },
  "timestamp": "2025-01-19T10:30:05Z"
}
```

---

## å¸¸è§é—®é¢˜

### Q1: LLM API è°ƒç”¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ
ä½¿ç”¨ `_fallback_analysis` é™çº§åˆ°è§„åˆ™åŒ¹é…ã€‚

### Q2: å¦‚ä½•é¿å…ç”»åƒå€¼è¶…å‡º 0-100 èŒƒå›´ï¼Ÿ
åœ¨ `update_profile` ä¸­ä½¿ç”¨ `max(0, min(100, value))`ã€‚

### Q3: å¦‚ä½•å¤„ç†å¹¶å‘è¯·æ±‚ï¼Ÿ
ä½¿ç”¨æ•°æ®åº“äº‹åŠ¡å’Œè¡Œé”ï¼š
```python
profile = db.query(UserProfile).filter(...).with_for_update().first()
```

### Q4: å¦‚ä½•æé«˜ LLM å“åº”é€Ÿåº¦ï¼Ÿ
- ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ (gpt-3.5-turbo)
- å‡å°‘ `max_tokens`
- å¼‚æ­¥è°ƒç”¨ + æµå¼è¿”å›

---

## ä¸‹ä¸€æ­¥

1. å®Œæˆå‰©ä½™ P0 æ¥å£
2. ç¼–å†™å•å…ƒæµ‹è¯•
3. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
4. ç›‘æ§ LLM API ä½¿ç”¨é‡

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-01-19
